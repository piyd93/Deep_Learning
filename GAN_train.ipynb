{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tflib.plot\n",
    "import utils\n",
    "import tflib as lib\n",
    "import tflib.linear\n",
    "import tflib.conv1d\n",
    "import models\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VirtualArgparse:\n",
    "    \n",
    "    # Path to dataset\n",
    "    #training_data = \"GAN_input.txt\"\n",
    "    training_data = \"final_lstm_op.txt\"\n",
    "    \n",
    "    # Name of directory to output\n",
    "    output_dir = \"pretrained\"\n",
    "    \n",
    "    save_every = 5000   #5000\n",
    "    iters = 20000   #200000\n",
    "    batch_size = 64\n",
    "    seq_length = 10\n",
    "    layer_dim = 128\n",
    "    critic_iters = 10\n",
    "    lamb = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = VirtualArgparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(args.output_dir):\n",
    "    os.makedirs(args.output_dir)\n",
    "\n",
    "if not os.path.isdir(os.path.join(args.output_dir, 'checkpoint')):\n",
    "    os.makedirs(os.path.join(args.output_dir, 'checkpoint'))\n",
    "\n",
    "if not os.path.isdir(os.path.join(args.output_dir, 'samples')):\n",
    "    os.makedirs(os.path.join(args.output_dir, 'samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 774592 lines in dataset\n"
     ]
    }
   ],
   "source": [
    "#load the dataset\n",
    "lines, charmp, inv_charmp = utils.load_dataset(\n",
    "    path=args.training_data,\n",
    "    max_length=args.seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters in dataset: 124\n"
     ]
    }
   ],
   "source": [
    "#store the charmap weigths\n",
    "with open(os.path.join(args.output_dir, 'charmap.pickle'), 'wb') as f:\n",
    "    pickle.dump(charmp, f)\n",
    "\n",
    "with open(os.path.join(args.output_dir, 'charmap_inv.pickle'), 'wb') as f:\n",
    "    pickle.dump(inv_charmp, f)\n",
    "    \n",
    "print(\"Number of unique characters in dataset: {}\".format(len(charmp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "real_inputs_discrete = tf.placeholder(tf.int32, shape=[args.batch_size, args.seq_length])\n",
    "real_inputs = tf.one_hot(real_inputs_discrete, len(charmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(64, 10), dtype=int32)\n",
      "Tensor(\"one_hot:0\", shape=(64, 10, 124), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#inputs to discriminator\n",
    "print(real_inputs_discrete)\n",
    "print(real_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1212 00:01:54.253783  3860 deprecation_wrapper.py:119] From C:\\Users\\Piyush's PC\\DLproject_2019\\models.py:64: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#input random noise to generator\n",
    "fake_inputs = models.Generator(args.batch_size, args.seq_length, args.layer_dim, len(charmp))\n",
    "fake_inputs_discrete = tf.argmax(fake_inputs, fake_inputs.get_shape().ndims-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape_2:0\", shape=(64, 10, 124), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(fake_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discriminator real and fake \n",
    "disc_real = models.Discriminator(real_inputs, args.seq_length, args.layer_dim, len(charmp))\n",
    "disc_fake = models.Discriminator(fake_inputs, args.seq_length, args.layer_dim, len(charmp))\n",
    "\n",
    "discriminator_cost = tf.reduce_mean(disc_fake) - tf.reduce_mean(disc_real)\n",
    "generator_cost = -tf.reduce_mean(disc_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1212 00:01:58.103499  3860 deprecation.py:323] From C:\\Users\\installations\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# WGAN lipschitz-penalty\n",
    "alpha = tf.random_uniform(\n",
    "    shape=[args.batch_size,1,1],\n",
    "    minval=0.,\n",
    "    maxval=1.\n",
    ")\n",
    "\n",
    "differences = fake_inputs - real_inputs\n",
    "interpolates = real_inputs + (alpha*differences)\n",
    "gradients = tf.gradients(models.Discriminator(interpolates, args.seq_length, args.layer_dim, len(charmp)), [interpolates])[0]\n",
    "slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1,2]))\n",
    "gradient_penalty = tf.reduce_mean((slopes-1.)**2)\n",
    "discriminator_cost += args.lamb * gradient_penalty\n",
    "\n",
    "gen_params = lib.params_with_name('Generator')\n",
    "disc_params = lib.params_with_name('Discriminator')\n",
    "\n",
    "gen_train_op = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.5, beta2=0.9).minimize(generator_cost, var_list=gen_params)\n",
    "disc_train_op = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.5, beta2=0.9).minimize(discriminator_cost, var_list=disc_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset iterator\n",
    "def inf_train_gen():\n",
    "    while True:\n",
    "        np.random.shuffle(lines)\n",
    "        for i in range(0, len(lines)-args.batch_size+1, args.batch_size):\n",
    "            yield np.array(\n",
    "                [[charmp[c] for c in l] for l in lines[i:i+args.batch_size]],\n",
    "                dtype='int32'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation set JSD for n=1: 0.004985971679623904\n",
      "validation set JSD for n=2: 0.0667152892948728\n",
      "validation set JSD for n=3: 0.3281906483195437\n",
      "validation set JSD for n=4: 0.6475340885547007\n"
     ]
    }
   ],
   "source": [
    "# During training we monitor JS divergence between the true & generated ngram\n",
    "# distributions for n=1,2,3,4. To get an idea of the optimal values, we\n",
    "# evaluate these statistics on a held-out set first.\n",
    "true_char_ngram_lms = [utils.NgramLanguageModel(i+1, lines[10*args.batch_size:], tokenize=False) for i in range(4)]\n",
    "validation_char_ngram_lms = [utils.NgramLanguageModel(i+1, lines[:10*args.batch_size], tokenize=False) for i in range(4)]\n",
    "for i in range(4):\n",
    "    print(\"validation set JSD for n={}: {}\".format(i+1, true_char_ngram_lms[i].js_with(validation_char_ngram_lms[i])))\n",
    "true_char_ngram_lms = [utils.NgramLanguageModel(i+1, lines, tokenize=False) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TensorFlow session...\n",
      "Local current time : Thu Dec 12 00:02:53 2019\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-7ca577fac596>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m             _disc_cost, _ = session.run(\n\u001b[0;32m     44\u001b[0m                 \u001b[1;33m[\u001b[0m\u001b[0mdiscriminator_cost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisc_train_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m                 \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mreal_inputs_discrete\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0m_data\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             )\n",
      "\u001b[1;32mC:\\Users\\installations\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\installations\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\installations\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\installations\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1354\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\installations\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1341\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\installations\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1431\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#training usually takes longer time for every 100 steps\n",
    "with tf.Session() as session:\n",
    "\n",
    "    # Time stamp\n",
    "    localtime = time.asctime( time.localtime(time.time()) )\n",
    "    print(\"Starting TensorFlow session...\")\n",
    "    print(\"Local current time :\", localtime)\n",
    "    \n",
    "    # Start TensorFlow session...\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    def generate_samples():\n",
    "        samples = session.run(fake_inputs)\n",
    "        samples = np.argmax(samples, axis=2)\n",
    "        decoded_samples = []\n",
    "        for i in range(len(samples)):\n",
    "            decoded = []\n",
    "            for j in range(len(samples[i])):\n",
    "                decoded.append(inv_charmp[samples[i][j]])\n",
    "            decoded_samples.append(tuple(decoded))\n",
    "        return decoded_samples\n",
    "\n",
    "    gen = inf_train_gen()\n",
    "\n",
    "    for iteration in range(args.iters + 1):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train Generator\n",
    "        if iteration > 0:\n",
    "            _ = session.run(gen_train_op)\n",
    "        \n",
    "        \n",
    "        # Train Discriminator\n",
    "        for i in range(args.critic_iters):\n",
    "            _data = next(gen)\n",
    "            \n",
    "            _gen_cost, _ =session.run(\n",
    "                [generator_cost,gen_train_op],\n",
    "            feed_dict={fake_inputs_discrete:_data}\n",
    "            \n",
    "            )\n",
    "                \n",
    "            _disc_cost, _ = session.run(\n",
    "                [discriminator_cost, disc_train_op],\n",
    "                feed_dict={real_inputs_discrete:_data }\n",
    "            \n",
    "            )\n",
    "          \n",
    "       \n",
    "        lib.plot.output_dir = args.output_dir\n",
    "        lib.plot.plot('time', time.time() - start_time)\n",
    "        lib.plot.plot('train generator cost', _gen_cost)\n",
    "        \n",
    "        \n",
    "        lib.plot.plot_generator('time', time.time() - start_time)\n",
    "        lib.plot.plot_generator('train discriminator cost', _disc_cost)\n",
    "\n",
    "        # Output to text file after every 100 samples\n",
    "        if iteration % 100 == 0 and iteration > 0:\n",
    "\n",
    "            samples = []\n",
    "            for i in range(10):\n",
    "                samples.extend(generate_samples())\n",
    "\n",
    "            for i in range(4):\n",
    "                lm = utils.NgramLanguageModel(i+1, samples, tokenize=False)\n",
    "                lib.plot.plot('js{}'.format(i+1), lm.js_with(true_char_ngram_lms[i]))\n",
    "\n",
    "            with open(os.path.join(args.output_dir, 'samples', 'samples_{}.txt').format(iteration), 'w') as f:\n",
    "                for s in samples:\n",
    "                    s = \"\".join(s)\n",
    "                    f.write(s + \"\\n\")\n",
    "                    \n",
    "                    \n",
    "        if iteration % args.save_every == 0 and iteration > 0:\n",
    "            model_saver = tf.train.Saver()\n",
    "            model_saver.save(session, os.path.join(args.output_dir, 'checkpoint', 'checkpoint_{}.ckpt').format(iteration))\n",
    "            print(\"{} / {} ({}%)\".format(iteration, args.iters, iteration/args.iters*100.0 ))\n",
    "\n",
    "        if iteration == args.iters:\n",
    "            print(\"...Training done.\")\n",
    "            \n",
    "           \n",
    "        \n",
    "        if iteration % 100 == 0:\n",
    "            \n",
    "            lib.plot.flush_generator()   \n",
    "            lib.plot.flush()\n",
    "        \n",
    "        lib.plot.tick()\n",
    "        \n",
    "        \n",
    "            \n",
    "# Time stamp\n",
    "localtime = time.asctime( time.localtime(time.time()) )\n",
    "print(\"Ending TensorFlow session.\")\n",
    "print(\"Local current time :\", localtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomString(stringLength=4):\n",
    "    \"\"\"Generate a random string of fixed length \"\"\"\n",
    "    letters = string.ascii_lowercase\n",
    "    return ''.join(random.choice(letters) for i in range(stringLength))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pretrained/samples/samples_3900.txt', 'r') as k:\n",
    "    keywords = k.read().splitlines()\n",
    "\n",
    "with open('rockyou_dataset.txt',errors='ignore') as f, open('output_23nov.txt', 'w') as o:\n",
    "    for line in f:\n",
    "        if any(key in line for key in keywords):\n",
    "            o.writelines(line)\n",
    "o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "with open('pretrained/samples/samples_3900.txt', 'r') as k:\n",
    "    keywords = k.read().splitlines()\n",
    "\n",
    "result.append(keywords)\n",
    "user = input()\n",
    "\n",
    "if  re.match(r\"{}.+\".format(user), keywords) in keywords:\n",
    "    with open('output_guess.txt', 'w') as o:\n",
    "        o.writeline(user)\n",
    "else:\n",
    "    print(\"add to lookup for next time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = open('output_23nov.txt', 'r')\n",
    "appnd=[]\n",
    "user = input()\n",
    "for line in out_file:\n",
    "    if re.match(r\"{}.+\".format(user), line) or len(line)==200:\n",
    "        print(\"recommendations are\",line)\n",
    "        \n",
    "        #appnd.append(line.rstrip('\\n')+keys)\n",
    "        appnd.append(line.rstrip('\\n')+keys)\n",
    "        #file_ip = [line.split(',') for line in thefile.readlines()]\n",
    "        #print(file_ip)\n",
    "        \n",
    "        #closeMatches(line, file_ip) \n",
    "        #newlist_words=[x.replace('\\n', '') for x in appnd]\n",
    "print(\"your input\", user)  \n",
    "\n",
    "print(appnd) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user=input()\n",
    "thefile = open(\"pretrained/samples/samples_1800.txt\", \"r\")\n",
    "appnd=[]\n",
    "\n",
    "keys=randomString()\n",
    "for line in thefile:\n",
    "    if re.match(r\"{}.+\".format(user), line) or len(line)==200:\n",
    "        print(\"recommendations are\",line)\n",
    "        \n",
    "        #appnd.append(line.rstrip('\\n')+keys)\n",
    "        appnd.append(line.rstrip('\\n')+keys)\n",
    "        #file_ip = [line.split(',') for line in thefile.readlines()]\n",
    "        #print(file_ip)\n",
    "        \n",
    "        #closeMatches(line, file_ip) \n",
    "        #newlist_words=[x.replace('\\n', '') for x in appnd]\n",
    "print(\"your input\", user)  \n",
    "\n",
    "print(appnd) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topgan = open(\"pretrained/samples/top10GAN.txt\", \"r\")\n",
    "#userip = open(\"pretrained/samples/userip.txt\",\"r\")\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "user = 'yars'\n",
    "count = 0\n",
    "list_best = []\n",
    "n_list=[]\n",
    "\n",
    "ganrange=topgan.readlines()\n",
    "for i in ganrange :\n",
    "#     for j in userip.readlines():\n",
    "\n",
    "    list_best.append(fuzz.ratio(user, i))\n",
    "        \n",
    "\n",
    "\n",
    "print(list_best)\n",
    "x_list=[]\n",
    "for count in range(0,len(ganrange)):\n",
    "    x_list.append(count)\n",
    "low_score = 20\n",
    "c=0\n",
    "for i in list_best : \n",
    "    if i > low_score : \n",
    "        c+= 1\n",
    "\n",
    "        \n",
    "maxpos = list_best.index(max(list_best)) \n",
    "\n",
    "topgan\n",
    "print (\"The numbers greater than low score : \" + str(c)) \n",
    "x=x_list\n",
    "y=list_best\n",
    "plt.xlabel(\"iter\")\n",
    "plt.ylabel(\"score\")\n",
    "plt.plot(x,y)\n",
    "\n",
    "print(\"done\")\n",
    "\n",
    "#epoch=range(0,len(list_best))\n",
    "\n",
    "\n",
    "plt.show()\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ganip = open(\"data/threethpasswords.txt\",\"r\",errors='ignore')\n",
    "ganop = open(\"pretrained/samples/sample6000.txt\", \"r\")\n",
    "\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "user = 'yars'\n",
    "count = 0\n",
    "list_best = []\n",
    "\n",
    "\n",
    "ganrange=ganip.readlines()\n",
    "for i in ganrange :\n",
    "    for j in ganop.readlines():\n",
    "\n",
    "        list_best.append(fuzz.ratio(j, i))\n",
    "        \n",
    "\n",
    "\n",
    "print(len(list_best))\n",
    "x_list=[]\n",
    "for count in range(0,len(ganrange)):\n",
    "    x_list.append(count)\n",
    "\n",
    "\n",
    "low_score = 50\n",
    "c=0\n",
    "for i in list_best : \n",
    "    if i > low_score : \n",
    "        c +=1\n",
    "        \n",
    "print (\"The numbers greater than low score : \" + str(c)) \n",
    "\n",
    "x=x_list\n",
    "y=list_best\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"score %\")\n",
    "plt.title('Loss plot (similarity score)')\n",
    "plt.plot(x,y)\n",
    "\n",
    "print(\"done\")\n",
    "\n",
    "#epoch=range(0,len(list_best))\n",
    "\n",
    "\n",
    "plt.show()\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pretrained/samples/sample6000.txt','r') as f:                                                                                                                                                                                                                                                 \n",
    "    distinct_content=set(f.readlines())                                                                                                                                                                                                                                                   \n",
    "\n",
    "to_file=\"\"                                                                                                                                                                                                                                                                       \n",
    "for element in distinct_content:                                                                                                                                                                                                                                                               \n",
    "    to_file=to_file+element                                                                                                                                                                                                                                                           \n",
    "with open('output_unique6k','w') as w:                                                                                                                                                                                                                                                  \n",
    "    w.write(to_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################unique words#######################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "def cos_similarity(textlist):\n",
    "    tfidf = TfidfVec.fit_transform(textlist)\n",
    "    return (tfidf * tfidf.T).toarray()\n",
    "cos_similarity(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.hist(Tfidf_scores, bins = 200)\n",
    "plt.xlim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest = tf.train.latest_checkpoint('pretrained/checkpoint/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edit dist\n",
    "import stringdist\n",
    "\n",
    "\n",
    "txt1 = open(\"data/threethpasswords.txt\",errors='ignore')\n",
    "txt2 = open(\"pretrained/samples/samples_500.txt\")\n",
    "\n",
    "stringdist.levenshtein(txt1, txt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#333 similar chars\n",
    "#51957\n",
    "with open('pretrained/samples/samples_1000.txt',errors='ignore') as infile:\n",
    "    lines=0\n",
    "    words=0\n",
    "    characters_1=0\n",
    "    for line in infile:\n",
    "        wordslist=line.split()\n",
    "        lines=lines+1\n",
    "        words=words+len(wordslist)\n",
    "        characters_1 += sum(len(word) for word in wordslist)\n",
    "print(lines)\n",
    "print(words)\n",
    "print(characters_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/threethpasswords.txt',errors='ignore') as infile:\n",
    "    lines=0\n",
    "    words=0\n",
    "    characters_2=0\n",
    "    for line in infile:\n",
    "        wordslist=line.split()\n",
    "        lines=lines+1\n",
    "        words=words+len(wordslist)\n",
    "        characters_2+= sum(len(word) for word in wordslist)\n",
    "print(lines)\n",
    "print(words)\n",
    "print(characters_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_characters = characters_1 + characters_2\n",
    "tot_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textdistance\n",
    "l1=open(\"pretrained/samples/samples_3900.txt\", \"r\") \n",
    "d1=l1.readlines()\n",
    "        \n",
    "l2=open(\"data/top100leakedwords.txt\",\"r\")\n",
    "\n",
    "d2=l2.readlines()\n",
    "# txt1 = open(\"data/threethpasswords.txt\",errors='ignore').read()\n",
    "# txt2 = open(\"pretrained/samples/samples_1000.txt\").read()\n",
    "\n",
    "score= textdistance.levenshtein( d2, d1)\n",
    "\n",
    "score_sim=textdistance.hamming.similarity(d1,d2)\n",
    "\n",
    "# from difflib import SequenceMatcher\n",
    "\n",
    "# val=similar(d1,d2)\n",
    "# print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import sklearn.cluster\n",
    "# import distance\n",
    "\n",
    "# words = \"apple\".split(\" \") #Replace this line\n",
    "# words = np.asarray(words) #So that indexing with a list will work\n",
    "# text_file = open(\"pretrained/samples/samples_500.txt\", \"r\")\n",
    "# lines = text_file.readlines()\n",
    "# print(lines)\n",
    "# print(len(lines))\n",
    "\n",
    "# lev_similarity = -1*np.array([[distance.levenshtein(w1,w2) for w1 in words] for w2 in text_file])\n",
    "# lev_similarity.reshape(-1,1)\n",
    "\n",
    "# affprop = sklearn.cluster.AffinityPropagation(affinity=\"precomputed\", damping=0.5)\n",
    "# affprop.fit(lev_similarity)\n",
    "# for cluster_id in np.unique(affprop.labels_):\n",
    "#     exemplar = words[affprop.cluster_centers_indices_[cluster_id]]\n",
    "#     cluster = np.unique(words[np.nonzero(affprop.labels_==cluster_id)])\n",
    "#     cluster_str = \", \".join(cluster)\n",
    "#     print(\" - *%s:* %s\" % (exemplar, cluster_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# N = 50\n",
    "# x = np.random.rand(score)\n",
    "# y = np.random.rand(tot_characters)\n",
    "\n",
    "# plt.scatter(x, y)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomString(stringLength=4):\n",
    "    \"\"\"Generate a random string of fixed length \"\"\"\n",
    "    letters = string.ascii_lowercase\n",
    "    return ''.join(random.choice(letters) for i in range(stringLength))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import difflib\n",
    "from difflib import get_close_matches \n",
    "import random \n",
    "import string\n",
    "user=input()\n",
    "thefile = open(\"pretrained/samples/samples_1800.txt\", \"r\")\n",
    "\n",
    "appnd=[]\n",
    "file_ip=[]\n",
    "keys=randomString()\n",
    "for line in thefile:\n",
    "    if re.match(r\"{}.+\".format(user), line) or len(line)==200:\n",
    "        print(\"recommendations are\",line)\n",
    "        \n",
    "        #appnd.append(line.rstrip('\\n')+keys)\n",
    "        appnd.append(line.rstrip('\\n')+keys)\n",
    "        #file_ip = [line.split(',') for line in thefile.readlines()]\n",
    "        #print(file_ip)\n",
    "        \n",
    "        #closeMatches(line, file_ip) \n",
    "        #newlist_words=[x.replace('\\n', '') for x in appnd]\n",
    "print(\"your input\", user)  \n",
    "\n",
    "print(appnd)     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(thefile.find(\"data/threethpasswords.txt\"))\n",
    "############################################################################################\n",
    "#print(thefile.findall(''.format(user),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textdistance\n",
    "l1=open(\"pretrained/samples/samples_1800.txt\", \"r\") \n",
    "d1=l1.readlines()\n",
    "        \n",
    "l2=open(\"data/top100leakedwords.txt\",\"r\")\n",
    "\n",
    "d2=l2.readlines()\n",
    "# txt1 = open(\"data/threethpasswords.txt\",errors='ignore').read()\n",
    "# txt2 = open(\"pretrained/samples/samples_1000.txt\").read()\n",
    "\n",
    "score= textdistance.levenshtein( d2, d1)\n",
    "score_sim = textdistance.hamming.similarity(d2,d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thefile = open(\"pretrained/samples/samples_1000.txt\", \"r\")\n",
    "\n",
    "\n",
    "# user = input() \n",
    "# print(str(user))\n",
    "# # returns first occurrence of Substring \n",
    "# result = thefile.find(user) \n",
    "# print (\"Substring  found at index:\", result ) \n",
    "\n",
    "\n",
    "# #fileNameOnly = thefile[:thefile.find(user)]\n",
    "# print(fileNameOnly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_english = open(\"data/threethpasswords.txt\",\"r\",errors = 'ignore')\n",
    "ganfile = open(\"pretrained/samples/samples_3900.txt\", \"r\")\n",
    "dict1=input_english.readlines()\n",
    "dict2 = ganfile.readlines()\n",
    "\n",
    "df = [ x for x in dict1 if x not in dict2 ]\n",
    "print(len(df))\n",
    "#print(thefile.find(\"data/threethpasswords.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"pretrained/charmap_inv.pickle\"\n",
    "with open(file, 'rb') as f1:  \n",
    "    lgr = pickle.load(f1)\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
